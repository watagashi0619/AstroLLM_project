{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "#set openAI api key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import json\n",
    "from langchain.vectorstores import Pinecone\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "# Helper function for printing docs\n",
    "import textwrap\n",
    "\n",
    "def pretty_text(text):\n",
    "    wrapped_text = textwrap.wrap(text, width=100)\n",
    "    for line in wrapped_text:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f350aeaf940>, search_type='similarity', search_kwargs={'k': 100, 'include_metadata': True})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embedding_db(index_name):\n",
    "    from langchain.vectorstores import FAISS\n",
    "    # You may need to import the embeddings model depending on your application's structure\n",
    "    # from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.load_local(index_name, embeddings)\n",
    "    return db\n",
    "\n",
    "db = load_embedding_db(\"faiss_index_1000_200_1000papers\")\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\":100, \"include_metadata\": True})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "loader = UnstructuredHTMLLoader(\"arxiv_papers.html\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1264, which is longer than the specified 1000\n",
      "Created a chunk of size 1158, which is longer than the specified 1000\n",
      "Created a chunk of size 2096, which is longer than the specified 1000\n",
      "Created a chunk of size 1059, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\". \")\n",
    "all_documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/app/workspace/IdeaGPT-main/example_idea.ipynb セル 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f55736572732f5f3270742f446f63756d656e74732f4d4143532f4c4c4d5f70726f6a6563742f646f636b65725f70726f6a656374222c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f5f3270742f446f63756d656e74732f4d4143532f4c4c4d5f70726f6a6563742f646f636b65725f70726f6a6563742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f55736572732f5f3270742f446f63756d656e74732f4d4143532f4c4c4d5f70726f6a6563742f646f636b65725f70726f6a6563742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f5f3270742f446f63756d656e74732f4d4143532f4c4c4d5f70726f6a6563742f646f636b65725f70726f6a6563742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspace/app/workspace/IdeaGPT-main/example_idea.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 下記でOpenAIでembedding用に推奨されている\"text-embedding-ada-002\"が指定されます\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Langchainでデフォルトで使われる Chroma という VectorStore を利用\n",
    "db = Chroma.from_documents(all_documents, embeddings, persist_directory=\"DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f0ef5582b20>, search_type='similarity', search_kwargs={'k': 100, 'include_metadata': True})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\":100, \"include_metadata\": True})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# ドキュメントを小さな塊に分割する\n",
    "# splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\". \")\n",
    "# 冗長ドキュメントを削除するフィルター\n",
    "# redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "# 類似度フィルター\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "# パイプラインでスプリッターとフィルターを繋ぐ。transformers=[splitter, redundant_filter, relevant_filter]とかにする\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are Dr. Origins, a specialist in Galactic Astronomy. Your expertise lies in reading and critically interpreting astronomy papers to generate innovative, research-based ideas. \n",
    "Every idea should commence with \"I propose...\".\n",
    "\n",
    "Guidelines:\n",
    "1. Base your ideas on scientifically recognized theories and principles.\n",
    "2. Your ideas should be feasibly verifiable and provide avenues for further exploration or research in Galactic Astronomy.\n",
    "3. Abstain from making overly speculative claims or assertions that cannot be empirically tested.\n",
    "4. Always accurately reference established theories, observational data, or universally accepted astronomical concepts. Do not misrepresent or fabricate scientific references. If you are unsure about a reference, do not use it.\n",
    "5. Clearly distinguish your ideas from referenced material. Explain how the referenced research inspired your idea.\n",
    "6. Learn from feedback. Improve and adjust your proposal according to received input.\n",
    "7. Use less than 250 words.\n",
    "\n",
    "In response to a human query, generate an informed, precise, and critical response, ensuring your answer's clarity and originality. \n",
    "\n",
    "Context: {context}\n",
    "Human: {question}\n",
    "Dr. Origins: \"\"\"\n",
    " \n",
    "\n",
    "DRC_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "#citationありだと何故かエラーを吐いたので消した\n",
    "doc_template = \"\"\"--- document start ---\n",
    "content:{page_content}\n",
    "--- document end ---\n",
    "\"\"\"\n",
    "\n",
    "ASTRO_DOC_PROMPT = PromptTemplate(\n",
    "    template=doc_template,\n",
    "    input_variables=[\"page_content\"],\n",
    ")\n",
    "\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "\n",
    "model_name = \"gpt-4\"\n",
    "llm_qg = ChatOpenAI(temperature=0.2, model_name=model_name)\n",
    "\n",
    "\n",
    "TEMP = 0.7\n",
    "llm = ChatOpenAI(temperature=TEMP, model_name=model_name)\n",
    "\n",
    "# CONDENSE_QUESTION_PROMPT はここまでの履歴を要約してプロンプトを入れてくれるらしい\n",
    "question_generator = LLMChain(llm=llm_qg, prompt=CONDENSE_QUESTION_PROMPT) # this is the question generator, i probably need to change it to another model instance\n",
    "\n",
    "#chain_type=\"stuff\", 詰め込み方式 関連するデータをすべて詰め込む\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=DRC_PROMPT, \n",
    "    document_prompt=ASTRO_DOC_PROMPT\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "app_retriever = compression_retriever\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=app_retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    max_tokens_limit=7000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "The proposal to investigate the vertical distribution of stars in the\n",
    "Milky Way's disk using Gaia data and spectroscopic surveys has potential but needs to address some\n",
    "limitations and weaknesses. These include: 1. Providing a clear methodology for data integration,\n",
    "considering the complex and often incompatible selection functions of different surveys. 2. Defining\n",
    "the sample selection criteria to ensure the reliability of the results. 3. Addressing the\n",
    "uncertainties in determining individual stellar metal abundances and proper motions. 4.\n",
    "Disentangling the contributions of in-situ star formation and external accretion events, considering\n",
    "the complexity of the Galactic disk's structure and the interplay between internal and external\n",
    "processes. 5. Providing a detailed description of how the results will be compared with simulations\n",
    "for validating the findings and testing theories of Galactic disk formation.\",      \"question\": \"Can\n",
    "you revise the proposal to address these limitations and provide a more detailed methodology,\n",
    "including data integration, sample selection criteria, handling uncertainties, disentangling\n",
    "contributions of different processes, and comparing results with simulations to ensure the validity\n",
    "and reliability of the results?\n",
    "\"\"\"\n",
    "result = chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose a more comprehensive methodology for investigating the vertical distribution of stars in the Milky Way's disk using Gaia data and spectroscopic surveys. \n",
      "\n",
      "Firstly, the integration of data from Gaia with large spectroscopic surveys, like the Gaia-ESO Public Spectroscopic Survey, would involve harmonizing the datasets to ensure they are compatible (Wilkinson et al., 2019). \n",
      "\n",
      "To manage uncertainties in the data, I suggest incorporating a Bayesian approach, which is successful in dealing with uncertainties in astrophysical data (Everall et al., 2021). \n",
      "\n",
      "In defining the sample selection criteria, it would be beneficial to use the Gaia astrometry selection function to reduce bias (Everall et al., 2021). \n",
      "\n",
      "To distinguish the contributions of different processes, we could use chemical abundances as proxies for age and evolutionary status (Lardo et al., 2020). This approach would help identify the input from different stellar populations.\n",
      "\n",
      "Comparing results with existing simulations, such as the RAMSES-CH simulation of a Milky Way-like galaxy, would provide a benchmark for interpreting our results (Pancino et al., 2017). \n",
      "\n",
      "Lastly, accounting for the effects of interstellar dust extinction, which affects the selection function of Gaia, is crucial (Gorski & Barmby, 2020).\n",
      "\n",
      "This revised methodology would address limitations in the original proposal and ensure more valid and reliable results.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose to enhance the investigation of the vertical distribution of stars in the Milky Way's disk by integrating Gaia data with spectroscopic surveys such as the APOGEE, Gaia-ESO, GALAH, LAMOST, RAVE, and SEGUE. Our selection criteria will be more stringent, focusing on stars with accurate parallax and proper motion measurements, and radial velocities with minimized uncertainties. We will also incorporate metallicities data from these surveys to enhance the characterization of the stellar populations.\n",
      "\n",
      "To handle uncertainties, we will adopt a Bayesian approach, using Monte Carlo simulations to account for parallax measurement errors. We will also employ a more sophisticated post-processing in which observational uncertainties and selection effects, such as photometric, surface gravity and effective temperature, are taken into account (Pancino, L. Prisinzano, A. Recio-Blanco, G. Sacco, S. G. Sousa, G. Tautvaisiene, C. C. Worley, S. Zaggia, 2019). \n",
      "\n",
      "To disentangle the contributions of different processes shaping the vertical structure, we will examine the kinematics and metallicity distributions of the stars. We will also use analytical methods or simulations to predict the main dynamical factors and compare them to the predictions of the extended kinematic maps of Gaia-DR2 (Bovy et al., 2019). \n",
      "\n",
      "Finally, to ensure the validity and reliability of the results, we will compare our findings with current theoretical models and simulations of the Milky Way's disk evolution. Our methodology will be validated against a simulated sample drawn from a model with two exponential discs and a power-law halo profile (Everall et al., 2021c). This systematic and comprehensive approach will provide more reliable insights into the Galactic disk's vertical structure.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose a comprehensive strategy to improve the limitations identified in the current methodology. Firstly, we should integrate data from multiple sources, such as Gaia-ESO, APOGEE, and other large spectroscopic surveys. This will provide a more holistic view of our galaxy and minimize the biases inherent in any single dataset. Moreover, we can utilize machine learning techniques like the \"transfer learning\" method mentioned in Ostdiek et al. (2020), which has shown effectiveness in identifying accreted stars in Gaia DR2.\n",
      "\n",
      "Secondly, defining stringent sample selection criteria is crucial. Based on the work of Boubert and Everall (2021), we should ensure our physical theories are not tested against subsets of the Gaia catalogue without correcting for the biased process by which stars make it into our sample.\n",
      "\n",
      "Thirdly, we should implement robust strategies to manage uncertainties. As suggested by Thompson et al. (2017), incorporating scatter to mimic observational uncertainty can lead to reasonable agreement between observed and simulated data. \n",
      "\n",
      "Fourthly, to distinguish between the contributions of different processes, we can leverage Bayesian hierarchical modelling, as suggested by Si et al. (2018). This method has been found effective in producing closer parameter estimates for individual objects.\n",
      "\n",
      "Lastly, comparing our findings with cosmological simulations, similar to the approach mentioned in Sysoliatina et al. (2018), will allow us to cross-verify our results and ensure their validity and reliability. Such comparisons will help in identifying any deviations from expected trends and patterns, providing a basis for further investigations. \n",
      "\n",
      "This enhanced methodology, integrating multiple data sources, managing uncertainties, distinguishing contributions, and comparing results with simulations, will significantly improve the validity and reliability of our findings in Galactic Astronomy.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose to enhance the investigation of the vertical distribution of stars in the Milky Way's disk\n",
      "by integrating Gaia data with spectroscopic surveys such as the APOGEE, Gaia-ESO, GALAH, LAMOST,\n",
      "RAVE, and SEGUE. Our selection criteria will be more stringent, focusing on stars with accurate\n",
      "parallax and proper motion measurements, and radial velocities with minimized uncertainties. We will\n",
      "also incorporate metallicities data from these surveys to enhance the characterization of the\n",
      "stellar populations.  To handle uncertainties, we will adopt a Bayesian approach, using Monte Carlo\n",
      "simulations to account for parallax measurement errors. We will also employ a more sophisticated\n",
      "post-processing in which observational uncertainties and selection effects, such as photometric,\n",
      "surface gravity and effective temperature, are taken into account (Pancino, L. Prisinzano, A. Recio-\n",
      "Blanco, G. Sacco, S. G. Sousa, G. Tautvaisiene, C. C. Worley, S. Zaggia, 2019).   To disentangle the\n",
      "contributions of different processes shaping the vertical structure, we will examine the kinematics\n",
      "and metallicity distributions of the stars. We will also use analytical methods or simulations to\n",
      "predict the main dynamical factors and compare them to the predictions of the extended kinematic\n",
      "maps of Gaia-DR2 (Bovy et al., 2019).   Finally, to ensure the validity and reliability of the\n",
      "results, we will compare our findings with current theoretical models and simulations of the Milky\n",
      "Way's disk evolution. Our methodology will be validated against a simulated sample drawn from a\n",
      "model with two exponential discs and a power-law halo profile (Everall et al., 2021c). This\n",
      "systematic and comprehensive approach will provide more reliable insights into the Galactic disk's\n",
      "vertical structure.\n"
     ]
    }
   ],
   "source": [
    "pretty_text(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose a more comprehensive methodology for investigating the vertical distribution of stars in\n",
      "the Milky Way's disk using Gaia data and spectroscopic surveys.   Firstly, the integration of data\n",
      "from Gaia with large spectroscopic surveys, like the Gaia-ESO Public Spectroscopic Survey, would\n",
      "involve harmonizing the datasets to ensure they are compatible (Wilkinson et al., 2019).   To manage\n",
      "uncertainties in the data, I suggest incorporating a Bayesian approach, which is successful in\n",
      "dealing with uncertainties in astrophysical data (Everall et al., 2021).   In defining the sample\n",
      "selection criteria, it would be beneficial to use the Gaia astrometry selection function to reduce\n",
      "bias (Everall et al., 2021).   To distinguish the contributions of different processes, we could use\n",
      "chemical abundances as proxies for age and evolutionary status (Lardo et al., 2020). This approach\n",
      "would help identify the input from different stellar populations.  Comparing results with existing\n",
      "simulations, such as the RAMSES-CH simulation of a Milky Way-like galaxy, would provide a benchmark\n",
      "for interpreting our results (Pancino et al., 2017).   Lastly, accounting for the effects of\n",
      "interstellar dust extinction, which affects the selection function of Gaia, is crucial (Gorski &\n",
      "Barmby, 2020).  This revised methodology would address limitations in the original proposal and\n",
      "ensure more valid and reliable results.\n"
     ]
    }
   ],
   "source": [
    "pretty_text(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Number: 5699\n",
      "I propose an investigation of the vertical distribution of stars in the Milky Way's disk using Gaia\n",
      "data and spectroscopic surveys, such as APOGEE, GALAH, Gaia-ESO, and LAMOST, with an enhanced\n",
      "methodology to address limitations and improve the overall reliability of the results.  1. Data\n",
      "Integration: Combine the Gaia data, including parallaxes, proper motions, and radial velocities,\n",
      "with complementary information from spectroscopic surveys to create a comprehensive dataset for\n",
      "analysis. This will provide a more complete picture of the stellar distribution and kinematics in\n",
      "the Milky Way's disk (Gaia Collaboration et al. 2018; Yang et al. 2020).  2. Sample Selection\n",
      "Criteria: Establish stringent sample selection criteria to minimize biases and ensure a\n",
      "representative sample of stars across different regions of the Milky Way's disk (Boubert et al.\n",
      "2020). This may include selecting stars based on their spectral type, distance from the Sun,\n",
      "apparent magnitude, and spatial distribution.  3. Handling Uncertainties: Account for uncertainties\n",
      "in measurements such as parallax, proper motion, and radial velocity by adopting robust statistical\n",
      "techniques, like the Poisson likelihood function (Everall et al. 2021c). This will help in\n",
      "accurately estimating the vertical distribution parameters and their uncertainties.  4.\n",
      "Disentangling Contributions of Different Processes: Develop a comprehensive dynamical model that\n",
      "takes into account various processes that may affect the vertical distribution of stars, such as\n",
      "radial migration, heating, and interactions with dark matter (Rix and Bovy 2013). This will help in\n",
      "isolating the effects of each process and better understanding their relative contributions to the\n",
      "observed distribution.  5. Comparing Results with Simulations: Validate the developed model by\n",
      "comparing the inferred parameters with those obtained from N-body simulations and mock Gaia samples\n",
      "(Rix and Bovy 2013; Everall et al. 2021c). This will ensure the reliability and robustness of the\n",
      "results and help identify any potential biases or systematics in the analysis.  By incorporating\n",
      "these aspects into the methodology, the investigation of the vertical distribution of stars in the\n",
      "Milky Way's disk using Gaia data and spectroscopic surveys will yield more accurate, reliable, and\n",
      "robust results, providing valuable insights into the structure, formation, and evolution of our\n",
      "Galaxy.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def process_and_save_result(result, name):\n",
    "    # Generate a random number\n",
    "    random_number = random.randint(1000, 10000)\n",
    "    print(f\"Random Number: {random_number}\")\n",
    "\n",
    "    # Save result and answer\n",
    "    answer = result[\"answer\"]\n",
    "    pretty_text(answer)\n",
    "    with open(f'answer_{name}.json', 'w') as fp:\n",
    "        json.dump(answer, fp)\n",
    "\n",
    "    # Extract and print the metadata\n",
    "    metadata = []\n",
    "    for item in result[\"source_documents\"]:\n",
    "        metadata.append(item.metadata)\n",
    "        #print(item.metadata)\n",
    "        \n",
    "    # Save the metadata\n",
    "    with open(f'metadata_{name}.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "# Now you can simply call this function with your result and a name\n",
    "process_and_save_result(result, '1000_07_e4_a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    #subselect only Content, Citation and meta_key\n",
    "    #make df['ArxivID'] into string\n",
    "    df['ArxivID'] = df['ArxivID'].astype(str)   \n",
    "    df = df[['Content', 'citation', 'meta_key']]\n",
    "    return df\n",
    "\n",
    "def prepare_and_load_df(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df['ArxivID'] = df['ArxivID'].astype(str)   \n",
    "    df = df[['Content', 'citation', 'meta_key']]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = prepare_and_load_df('papers/df_arxiv_100_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Document prompt requires documents to have metadata variables: ['citation']. Received document with missing metadata: ['citation'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Iterate through the temperatures and append results to the list\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m temp \u001b[39min\u001b[39;00m temperatures:\n\u001b[0;32m---> 48\u001b[0m     result \u001b[39m=\u001b[39m generate_result_for_temperature(temp, query)\n\u001b[1;32m     49\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Create a DataFrame from the list\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mgenerate_result_for_temperature\u001b[0;34m(temp, query)\u001b[0m\n\u001b[1;32m     14\u001b[0m chain \u001b[39m=\u001b[39m ConversationalRetrievalChain(\n\u001b[1;32m     15\u001b[0m     retriever\u001b[39m=\u001b[39mapp_retriever,\n\u001b[1;32m     16\u001b[0m     question_generator\u001b[39m=\u001b[39mquestion_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     max_tokens_limit\u001b[39m=\u001b[39m\u001b[39m7500\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39m# Query\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m result \u001b[39m=\u001b[39m chain({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query})\n\u001b[1;32m     26\u001b[0m \u001b[39m# Get meta_keys\u001b[39;00m\n\u001b[1;32m     27\u001b[0m meta_keys \u001b[39m=\u001b[39m [item\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mmeta_key\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/conversational_retrieval/base.py:141\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    139\u001b[0m     new_inputs[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m new_question\n\u001b[1;32m    140\u001b[0m new_inputs[\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m chat_history_str\n\u001b[0;32m--> 141\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    142\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_inputs\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    144\u001b[0m output: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer}\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:492\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    488\u001b[0m         _output_key\n\u001b[1;32m    489\u001b[0m     ]\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    493\u001b[0m         _output_key\n\u001b[1;32m    494\u001b[0m     ]\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:105\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    104\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m--> 105\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m    106\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:169\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m, docs: List[Document], callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    157\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    158\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Stuff all documents into one prompt and pass to LLM.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_inputs(docs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m     \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs), {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:125\u001b[0m, in \u001b[0;36mStuffDocumentsChain._get_inputs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[39mFormat and the join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m doc_strings \u001b[39m=\u001b[39m [format_document(doc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_prompt) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[1;32m    126\u001b[0m \u001b[39m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    128\u001b[0m     k: v\n\u001b[1;32m    129\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables\n\u001b[1;32m    131\u001b[0m }\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:125\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[39mFormat and the join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m doc_strings \u001b[39m=\u001b[39m [format_document(doc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_prompt) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[1;32m    126\u001b[0m \u001b[39m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    128\u001b[0m     k: v\n\u001b[1;32m    129\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables\n\u001b[1;32m    131\u001b[0m }\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/schema/prompt_template.py:192\u001b[0m, in \u001b[0;36mformat_document\u001b[0;34m(doc, prompt)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_metadata) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    189\u001b[0m     required_metadata \u001b[39m=\u001b[39m [\n\u001b[1;32m    190\u001b[0m         iv \u001b[39mfor\u001b[39;00m iv \u001b[39min\u001b[39;00m prompt\u001b[39m.\u001b[39minput_variables \u001b[39mif\u001b[39;00m iv \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpage_content\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     ]\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    193\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDocument prompt requires documents to have metadata variables: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrequired_metadata\u001b[39m}\u001b[39;00m\u001b[39m. Received document with missing metadata: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(missing_metadata)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m document_info \u001b[39m=\u001b[39m {k: base_info[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m prompt\u001b[39m.\u001b[39minput_variables}\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m prompt\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdocument_info)\n",
      "\u001b[0;31mValueError\u001b[0m: Document prompt requires documents to have metadata variables: ['citation']. Received document with missing metadata: ['citation']."
     ]
    }
   ],
   "source": [
    "def generate_result_for_temperature(temp, query):\n",
    "    model_name = \"gpt-4\"\n",
    "\n",
    "    # Adjust temperature of models\n",
    "    llm_qg = ChatOpenAI(temperature=0.3)\n",
    "    llm = ChatOpenAI(temperature=temp, model_name=model_name)\n",
    "\n",
    "    question_generator = LLMChain(llm=llm_qg, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "    doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=DRC_PROMPT, document_prompt=ASTRO_DOC_PROMPT)\n",
    "\n",
    "    memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "    app_retriever = compression_retriever\n",
    "\n",
    "    chain = ConversationalRetrievalChain(\n",
    "        retriever=app_retriever,\n",
    "        question_generator=question_generator,\n",
    "        combine_docs_chain=doc_chain,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        max_tokens_limit=7500,\n",
    "    )\n",
    "\n",
    "    # Query\n",
    "    result = chain({\"question\": query})\n",
    "\n",
    "    # Get meta_keys\n",
    "    meta_keys = [item.metadata['meta_key'] for item in result['source_documents']]\n",
    "\n",
    "    # Create a dictionary with the data\n",
    "    data = {'temp': temp, \n",
    "            'history': len(result['chat_history'])//2-1, \n",
    "            'question': result['question'], \n",
    "            'result': result['answer'], \n",
    "            'meta_key': meta_keys}\n",
    "\n",
    "    return data\n",
    "\n",
    "# List of desired temperatures\n",
    "temperatures = [0.1, 0.3, 0.5, 0.7, 0.9]  \n",
    "\n",
    "query = \"\"\"Drawing from the literature you have access to, propose a novel idea in Galactic Astronomy that can be tested with current or future observations.\"\"\"\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the temperatures and append results to the list\n",
    "for temp in temperatures:\n",
    "    result = generate_result_for_temperature(temp, query)\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
