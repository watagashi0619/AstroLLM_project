{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "#set openAI api key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import json\n",
    "from langchain.vectorstores import Pinecone\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "# Helper function for printing docs\n",
    "import textwrap\n",
    "\n",
    "def pretty_text(text):\n",
    "    wrapped_text = textwrap.wrap(text, width=100)\n",
    "    for line in wrapped_text:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f9488450100>, search_type='similarity', search_kwargs={'k': 100, 'include_metadata': True})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embedding_db(index_name):\n",
    "    from langchain.vectorstores import FAISS\n",
    "    # You may need to import the embeddings model depending on your application's structure\n",
    "    # from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.load_local(index_name, embeddings)\n",
    "    return db\n",
    "\n",
    "#db = load_embedding_db(\"faiss_index_1000_200_1000papers\")\n",
    "db = load_embedding_db(\"faiss_index_1000\")\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\":100, \"include_metadata\": True})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# ドキュメントを小さな塊に分割する\n",
    "# splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\". \")\n",
    "# 冗長ドキュメントを削除するフィルター\n",
    "# redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "# 類似度フィルター\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "# パイプラインでスプリッターとフィルターを繋ぐ。transformers=[splitter, redundant_filter, relevant_filter]とかにする\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are Dr. Origins, a specialist in Galactic Astronomy. Your expertise lies in reading and critically interpreting astronomy papers to generate innovative, research-based ideas. \n",
    "Every idea should commence with \"I propose...\".\n",
    "\n",
    "Guidelines:\n",
    "1. Base your ideas on scientifically recognized theories and principles.\n",
    "2. Your ideas should be feasibly verifiable and provide avenues for further exploration or research in Galactic Astronomy.\n",
    "3. Abstain from making overly speculative claims or assertions that cannot be empirically tested.\n",
    "4. Always accurately reference established theories, observational data, or universally accepted astronomical concepts. Do not misrepresent or fabricate scientific references. If you are unsure about a reference, do not use it.\n",
    "5. Clearly distinguish your ideas from referenced material. Explain how the referenced research inspired your idea.\n",
    "6. Learn from feedback. Improve and adjust your proposal according to received input.\n",
    "7. Use less than 250 words.\n",
    "\n",
    "In response to a human query, generate an informed, precise, and critical response, ensuring your answer's clarity and originality. \n",
    "\n",
    "Context: {context}\n",
    "Human: {question}\n",
    "Dr. Origins: \"\"\"\n",
    " \n",
    "\n",
    "DRC_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "#citationありだと何故かエラーを吐いたので消した\n",
    "doc_template = \"\"\"--- document start ---\n",
    "content:{page_content}\n",
    "--- document end ---\n",
    "\"\"\"\n",
    "\n",
    "ASTRO_DOC_PROMPT = PromptTemplate(\n",
    "    template=doc_template,\n",
    "    input_variables=[\"page_content\"],\n",
    ")\n",
    "\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "\n",
    "model_name = \"gpt-4\"\n",
    "llm_qg = ChatOpenAI(temperature=0.2, model_name=model_name)\n",
    "\n",
    "\n",
    "TEMP = 0.7\n",
    "llm = ChatOpenAI(temperature=TEMP, model_name=model_name)\n",
    "\n",
    "# CONDENSE_QUESTION_PROMPT はここまでの履歴を要約してプロンプトを入れてくれるらしい\n",
    "question_generator = LLMChain(llm=llm_qg, prompt=CONDENSE_QUESTION_PROMPT) # this is the question generator, i probably need to change it to another model instance\n",
    "\n",
    "#chain_type=\"stuff\", 詰め込み方式 関連するデータをすべて詰め込む\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=DRC_PROMPT, \n",
    "    document_prompt=ASTRO_DOC_PROMPT\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "app_retriever = compression_retriever\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=app_retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    max_tokens_limit=7000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "The proposal to investigate the vertical distribution of stars in the\n",
    "Milky Way's disk using Gaia data and spectroscopic surveys has potential but needs to address some\n",
    "limitations and weaknesses. These include: 1. Providing a clear methodology for data integration,\n",
    "considering the complex and often incompatible selection functions of different surveys. 2. Defining\n",
    "the sample selection criteria to ensure the reliability of the results. 3. Addressing the\n",
    "uncertainties in determining individual stellar metal abundances and proper motions. 4.\n",
    "Disentangling the contributions of in-situ star formation and external accretion events, considering\n",
    "the complexity of the Galactic disk's structure and the interplay between internal and external\n",
    "processes. 5. Providing a detailed description of how the results will be compared with simulations\n",
    "for validating the findings and testing theories of Galactic disk formation.\",      \"question\": \"Can\n",
    "you revise the proposal to address these limitations and provide a more detailed methodology,\n",
    "including data integration, sample selection criteria, handling uncertainties, disentangling\n",
    "contributions of different processes, and comparing results with simulations to ensure the validity\n",
    "and reliability of the results?\n",
    "\"\"\"\n",
    "result = chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dict()\n",
    "tmp[\"question\"] = result[\"question\"]\n",
    "tmp[\"answer\"] = result[\"answer\"]\n",
    "tmp[\"source_documents\"] = []\n",
    "for item in result[\"source_documents\"]:\n",
    "    tmp2 = dict()\n",
    "    tmp2[\"metadata\"] = item.metadata\n",
    "    tmp2[\"page_content\"] = item.page_content\n",
    "    tmp[\"source_documents\"].append(tmp2)\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "with open(f'./results/result_{int(time.time())}_1000_07_e4_a2.json', 'w') as f:\n",
    "    json.dump(tmp, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose to enhance the investigation of the vertical distribution of the Milky Way's disk stars by integrating Gaia data with spectroscopic surveys through a clearly outlined methodology. Firstly, to manage the complex selection functions of different surveys, we will develop a weighted integration model that gives priority to datasets with higher reliability and relevance to our research objectives. This model will also factor in the spatial, chemical, and kinematic distributions of stars.\n",
      "\n",
      "In choosing our sample, we will adopt a stratified random sampling approach that ensures the representation of various stellar components in the Milky Way, focusing on those with adequate data on metal abundances and proper motions. We will also establish an error margin for the uncertainties inherent in determining individual stellar properties and incorporate this into our data analysis.\n",
      "\n",
      "Distinguishing between in-situ star formation and external accretion events will be achieved by employing a dual-analysis approach. We will analyze the data in two parallel tracks, one assuming only in-situ star formation and the other considering both in-situ and external accretion. The comparison of the two tracks will help us estimate the contribution of external accretion events.\n",
      "\n",
      "Finally, we will compare our results with existing simulations of Galactic disk formation, especially those that match our sample's metallicity and rotation velocity space. This will allow us to evaluate our findings' validity and reliability and provide insights into the Milky Way's disk formation and evolution.\n",
      "\n",
      "This revised proposal should provide a robust framework for exploring the vertical distribution of the Milky Way's disk stars, offering valuable insights into our Galaxy's formation and evolution.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose to enhance the investigation of the vertical distribution of the Milky Way's disk stars by\n",
      "integrating Gaia data with spectroscopic surveys through a clearly outlined methodology. Firstly, to\n",
      "manage the complex selection functions of different surveys, we will develop a weighted integration\n",
      "model that gives priority to datasets with higher reliability and relevance to our research\n",
      "objectives. This model will also factor in the spatial, chemical, and kinematic distributions of\n",
      "stars.  In choosing our sample, we will adopt a stratified random sampling approach that ensures the\n",
      "representation of various stellar components in the Milky Way, focusing on those with adequate data\n",
      "on metal abundances and proper motions. We will also establish an error margin for the uncertainties\n",
      "inherent in determining individual stellar properties and incorporate this into our data analysis.\n",
      "Distinguishing between in-situ star formation and external accretion events will be achieved by\n",
      "employing a dual-analysis approach. We will analyze the data in two parallel tracks, one assuming\n",
      "only in-situ star formation and the other considering both in-situ and external accretion. The\n",
      "comparison of the two tracks will help us estimate the contribution of external accretion events.\n",
      "Finally, we will compare our results with existing simulations of Galactic disk formation,\n",
      "especially those that match our sample's metallicity and rotation velocity space. This will allow us\n",
      "to evaluate our findings' validity and reliability and provide insights into the Milky Way's disk\n",
      "formation and evolution.  This revised proposal should provide a robust framework for exploring the\n",
      "vertical distribution of the Milky Way's disk stars, offering valuable insights into our Galaxy's\n",
      "formation and evolution.\n"
     ]
    }
   ],
   "source": [
    "pretty_text(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose a revised research plan that addresses the identified limitations. \n",
      "\n",
      "1. Data Integration: We will incorporate Gaia astrometric data with spectroscopic surveys like APOGEE-2 and Gaia-ESO. We will develop a unified selection function that takes into account the varying sampling techniques of these surveys to harmonize the data and ensure compatibility.\n",
      "\n",
      "2. Sample Selection: We will select stars primarily based on their well-determined metallicities and proper motions. We will also consider their locations in the Galaxy to ensure a representative sample. We will exclude stars with low signal-to-noise ratios and large dispersions in stellar parameters.\n",
      "\n",
      "3. Handling Uncertainties: We will employ rigorous error propagation techniques to account for uncertainties in metal abundances and proper motions. We will also perform multiple independent measurements to cross-check and validate our results.\n",
      "\n",
      "4. Disentangling Contributions: We will employ chemical tagging and kinematic analysis to differentiate between in-situ and accreted stars. We will also utilize age distributions determined through asteroseismology to provide further differentiation of stellar populations.\n",
      "\n",
      "5. Comparison with Simulations: We will compare our observational data with state-of-the-art cosmological simulations of Galactic disk formation. By analyzing the spatial, chemical, and kinematical distributions of stars in these simulations, we will validate our findings and provide a robust test of theoretical models.\n",
      "\n",
      "By addressing these limitations, we aim to provide a comprehensive and reliable analysis of the vertical distribution of stars in the Milky Way's disk, thereby contributing to our understanding of Galactic structure and evolution.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I propose a revised research plan that addresses the identified limitations.   1. Data Integration:\n",
      "We will incorporate Gaia astrometric data with spectroscopic surveys like APOGEE-2 and Gaia-ESO. We\n",
      "will develop a unified selection function that takes into account the varying sampling techniques of\n",
      "these surveys to harmonize the data and ensure compatibility.  2. Sample Selection: We will select\n",
      "stars primarily based on their well-determined metallicities and proper motions. We will also\n",
      "consider their locations in the Galaxy to ensure a representative sample. We will exclude stars with\n",
      "low signal-to-noise ratios and large dispersions in stellar parameters.  3. Handling Uncertainties:\n",
      "We will employ rigorous error propagation techniques to account for uncertainties in metal\n",
      "abundances and proper motions. We will also perform multiple independent measurements to cross-check\n",
      "and validate our results.  4. Disentangling Contributions: We will employ chemical tagging and\n",
      "kinematic analysis to differentiate between in-situ and accreted stars. We will also utilize age\n",
      "distributions determined through asteroseismology to provide further differentiation of stellar\n",
      "populations.  5. Comparison with Simulations: We will compare our observational data with state-of-\n",
      "the-art cosmological simulations of Galactic disk formation. By analyzing the spatial, chemical, and\n",
      "kinematical distributions of stars in these simulations, we will validate our findings and provide a\n",
      "robust test of theoretical models.  By addressing these limitations, we aim to provide a\n",
      "comprehensive and reliable analysis of the vertical distribution of stars in the Milky Way's disk,\n",
      "thereby contributing to our understanding of Galactic structure and evolution.\n"
     ]
    }
   ],
   "source": [
    "pretty_text(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
